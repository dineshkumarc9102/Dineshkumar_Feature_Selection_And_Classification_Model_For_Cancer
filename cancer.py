# -*- coding: utf-8 -*-
"""Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__s0VAcT8jmIveX-N-1K8PKO9UuF3GQY
"""

# Load libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
import seaborn as sns

cc= pd.read_csv("B:\SEM 6\CAPSTONE PROJECT\DATASETf.csv")

missing_values=["?"]
cc = pd.read_csv("B:\SEM 6\CAPSTONE PROJECT\DATASETf.csv",na_values=missing_values)

cc=cc.dropna()

cc

# Split-out validation dataset
array = cc.values
X = array[:,0:7]
y = array[:,7]

feature_cols = ['Age','IUD','IUD(years)','Hinselmann','Schiller','Citology','Biopsy']
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) # 70% training and 30% test

#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)

# Commented out IPython magic to ensure Python compatibility.
feature_imp = pd.Series(clf.feature_importances_,index=feature_cols).sort_values(ascending=False)
feature_imp
# %matplotlib inline
# Creating a bar plot
plt.figure()
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features in dataset 1")
plt.show()









# Load libraries
import pandas as pd
import warnings
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
warnings.filterwarnings('ignore')

# Split-out validation dataset
array = cc.values
X = array[:,0:7]
Y = array[:,7]
validation_size = 0.30
seed = 4

X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size,
random_state=seed)
scoring = 'accuracy'
models = []
models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto')))
models.append(('RF', RandomForestClassifier(n_estimators=70)))
# evaluate each model in turn
results = []
names = []

for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed,shuffle=True)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)

print("knn based confusion matrix")
knn = KNeighborsClassifier()
knn.fit(X_train, Y_train)
predictions = knn.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("Decision tree based confusion matrix")
dtc=DecisionTreeClassifier()
dtc.fit(X_train, Y_train)
predictions = dtc.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("Logistic regression based confusion matrix")
lg=LogisticRegression(solver='liblinear', multi_class='ovr')
lg.fit(X_train, Y_train)
predictions = lg.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("SVM based confusion matrix")
sv=SVC(gamma='auto')
sv.fit(X_train, Y_train)
predictions = sv.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("LinearDiscriminant based confusion matrix")
ld=LinearDiscriminantAnalysis()
ld.fit(X_train, Y_train)
predictions = ld.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("Gaussian NB based confusion matrix")
gb=GaussianNB()
gb.fit(X_train, Y_train)
predictions = gb.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("Neural net based confusion matrix")
mlp = MLPClassifier(hidden_layer_sizes=(31,31),max_iter=100)
mlp.fit(X_train,Y_train)
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto',
              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(31, 31), learning_rate='constant',
       learning_rate_init=0.001, max_iter=100, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
predictions = mlp.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))

print("Random Forest based confusion matrix")
rf=RandomForestClassifier(n_estimators=100)
rf.fit(X_train, Y_train)
predictions = rf.predict(X_validation)
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))





